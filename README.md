# ğŸ“„ RAG Document Search â€” AI-Powered PDF Question Answering

**An intelligent Retrieval-Augmented Generation (RAG) application that allows users to upload a PDF and ask natural-language questions â€” with answers generated strictly from the document itself.**

Built using **LangChain**, **LangGraph**, and modern LLM tooling, this project demonstrates how AI can transform static documents into interactive knowledge systems.

---

## ğŸ“‹ Overview

This project implements a Retrieval-Augmented Generation pipeline designed for accuracy and traceability. Instead of relying on general knowledge, it answers questions by referencing specific data within your uploaded files.

**The Pipeline:**
1.  ğŸ“ **Ingests a PDF**
2.  âœ‚ **Breaks it into meaningful chunks**
3.  ğŸ§  **Converts text into vector embeddings**
4.  ğŸ” **Retrieves relevant sections**
5.  ğŸ¤– **Uses an LLM to answer questions**
6.  âœ… **Ensures answers ONLY come from the document**

It utilizes a **graph-based architecture (LangGraph)**, making the pipeline modular, traceable, and production-ready.

> **Think of it as ChatGPT â€” but limited to your own document.**

---

## âœ¨ Features

* **Document-aware Question Answering:** Contextual answers derived solely from source material.
* **Zero Hallucinations:** Strict adherence to the provided document context.
* **Graph-based RAG Pipeline:** Structured flow using LangGraph for better control.
* **Modular Code Architecture:** Separation of concerns (State, Nodes, Vector Store).
* **Efficient Vector Search:** fast retrieval of semantic chunks.
* **Supports Multiple Document Types:** Extensible design.
* **Easy to Extend & Customize:** Built for developers.

---

## ğŸ§  Tech Stack

* **Language:** Python
* **Orchestration:** LangChain, LangGraph
* **Embeddings:** [OpenAI / HuggingFace] Embeddings
* **Vector Store:** FAISS / ChromaDB
* **LLM Provider:** [OpenAI GPT / Llama / etc.]
* **Frontend:** Streamlit

---

## ğŸ“‚ Project Structure

```bash
RAG_Document_Search
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Dhruta_resume.pdf
â”‚   â””â”€â”€ url.txt
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_state.py
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ vectorstore.py
â”‚   â”‚
â”‚   â”œâ”€â”€ graph_builder/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ graph_builder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ node/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ nodes.py
â”‚   â”‚   â””â”€â”€ reactnode.py
â”‚   â”‚
â”‚   â””â”€â”€ document_ingestion/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ document_process.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ streamlit_app.py
â””â”€â”€ requirements.txt

## ğŸ” How It Works

1. **Upload a PDF:** The document is read and converted to clean text.
2. **Chunking & Embeddings:** The text is split into manageable pieces and embedded into vectors.
3. **Vector Indexing:** Chunks are stored in a vector database for semantic search.
4. **User Asks a Question:** The system retrieves only the chunks relevant to the query.
5. **RAG-Based Answering:** The LLM synthesizes an answer using the retrieved context.

**Target:**
* ğŸ¯ No external data
* ğŸ¯ No hallucinations
* ğŸ¯ Reliable context-aware answers

---

## ğŸ—ï¸ Architecture (LangGraph Workflow)

The system runs as a stateful graph pipeline consisting of the following nodes:

ğŸ”¹ **Document Loader Node:** Handles file ingestion.
ğŸ”¹ **Chunk Processor Node:** Splits text for processing.
ğŸ”¹ **Embedding Node:** Generates vector representations.
ğŸ”¹ **Vector Retrieval Node:** Fetches context based on user queries.
ğŸ”¹ **LLM Response Node:** Generates the final answer.

Benefits of this architecture:**
âœ¨ **Traceable:** You can visualize the path of execution.
âœ¨ **Scalable:** Easy to add new nodes (e.g., a grading node or web search node).
âœ¨ **Production-ready:** State management is built-in.
--

## ğŸš€ Getting Started
## Prerequisites
 Python 3.9+
 API Key for LLM Provider (e.g., `OPENAI_API_KEY`)
### Installation

1. **Clone the repository**
   ```bash
   git clone [https://github.com/yourusername/RAG_Document_Search.git](https://github.com/yourusername/RAG_Document_Search.git)
   cd RAG_Document_Search