ğŸ“„ RAG Document Search â€” AI-Powered PDF Question Answering

An intelligent Retrieval-Augmented Generation (RAG) application that allows users to upload a PDF and ask natural-language questions â€” with answers generated strictly from the document itself. Built using LangChain, LangGraph, and modern LLM tooling, this project demonstrates how AI can transform static documents into interactive knowledge systems.

ğŸ“‹ Overview

This project implements a Retrieval-Augmented Generation pipeline that:

ğŸ“ Ingests a PDF
âœ‚ Breaks it into meaningful chunks
ğŸ§  Converts text into vector embeddings
ğŸ” Retrieves relevant sections
ğŸ¤– Uses an LLM to answer questions
âœ… Ensures answers ONLY come from the document

It follows a graph-based architecture using LangGraph â€” making the pipeline modular, traceable, and production-ready.

Think of it as ChatGPT â€” but limited to your own document.

âœ¨ Features
	â€¢	Document-aware Question Answering
	â€¢	Zero Hallucinations
	â€¢	Graph-based RAG Pipeline
	â€¢	Modular Code Architecture
	â€¢	Efficient Vector Search
	â€¢	Supports Multiple Document Types
	â€¢	Easy to Extend & Customize

ğŸ§  Tech Stack
	â€¢	Python
	â€¢	LangChain
	â€¢	LangGraph
	â€¢	Embeddings Model
	â€¢	Vector Store (FAISS / Similar)
	â€¢	LLM Provider
	â€¢	Document Loaders


RAG_Document_Search
â”‚
â”œâ”€â”€ data/                    
â”‚   â”œâ”€â”€ Dhruta_resume.pdf
â”‚   â””â”€â”€ url.txt
â”‚
â”œâ”€â”€ src/                      
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_state.py
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ vectorstore.py
â”‚   â”‚
â”‚   â”œâ”€â”€ graph_builder/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ graph_builder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ node/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ nodes.py
â”‚   â”‚   â””â”€â”€ reactnode.py
â”‚   â”‚
â”‚   â””â”€â”€ document_ingestion/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ document_process.py
|___main.py
|___streamlit_app.py
|___requirements.txt


ğŸ” How It Works

1ï¸âƒ£ Upload a PDF
The document is read and converted to clean text

2ï¸âƒ£ Chunking & Embeddings
The text is split and embedded

3ï¸âƒ£ Vector Indexing
Chunks are stored for semantic search

4ï¸âƒ£ User Asks a Question
Relevant chunks are retrieved

5ï¸âƒ£ RAG-Based Answering
The LLM responds using only your document

ğŸ¯ No external data
ğŸ¯ No hallucinations
ğŸ¯ Reliable context-aware answers


ğŸ—ï¸ Architecture (LangGraph Workflow)

The system runs as a graph pipeline consisting of:

ğŸ”¹ Document Loader Node
ğŸ”¹ Chunk Processor Node
ğŸ”¹ Embedding Node
ğŸ”¹ Vector Retrieval Node
ğŸ”¹ LLM Response Node

This makes the flow:

âœ¨ Traceable
âœ¨ Scalable
âœ¨ Production-ready
